"""
æ™ºèƒ½æœç´¢åŠ©æ‰‹
LangGraph + Tavily API
"""

import asyncio
import json
import os
import datetime
from typing import TypedDict, Annotated, List, Optional, Literal

from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from pydantic import BaseModel, Field
from tavily import TavilyClient

load_dotenv()

def get_current_date():
    return datetime.datetime.now().strftime("%Y-%m-%d %A")

class QueryAnalysis(BaseModel):
    """ç”¨æˆ·æŸ¥è¯¢åˆ†æç»“æœ"""
    summary: str = Field(description="ç”¨æˆ·æŸ¥è¯¢çš„ç®€è¦æ€»ç»“")
    search_query: str = Field(description="ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢è¯(Query)")
    needs_search: bool = Field(description="æ˜¯å¦éœ€è¦è”ç½‘æœç´¢ä»¥è·å–ç­”æ¡ˆ", default=True)
    is_exit: bool = Field(
        description="ç”¨æˆ·æ˜¯å¦è¡¨è¾¾äº†ç»“æŸå¯¹è¯ã€å†è§æˆ–ç¦»å¼€çš„æ„å›¾",
        default=False
    )

class SearchState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    analysis: Optional[QueryAnalysis]
    search_context: str
    step: str

llm = ChatOpenAI(
    model=os.getenv("MODEL"),
    api_key=os.getenv("API_KEY"),
    base_url=os.getenv("BASE_URL"),
    temperature=0.5,
)

tavily_api_key = os.getenv("TAVILY_API_KEY")
if not tavily_api_key:
    raise ValueError("TAVILY_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
tavily_client = TavilyClient(api_key=tavily_api_key)

# Prompts
# ===================================================

PROMPT_ANALYZE = """
ä½ æ˜¯ä¸€ä¸ªæœç´¢æ„å›¾åˆ†æä¸“å®¶ã€‚
å½“å‰æ—¶é—´æ˜¯ï¼š{current_date}
è¯·åˆ†æç”¨æˆ·çš„æœ€æ–°è¾“å…¥ã€‚

è¦æ±‚ï¼š
1. é˜…è¯»å®Œæ•´çš„å¯¹è¯å†å²ã€‚
2. å¦‚æœç”¨æˆ·æ˜¯åœ¨é—²èŠï¼Œneeds_search è®¾ä¸º falseã€‚
3. å¦‚æœç”¨æˆ·åœ¨è¯¢é—®äº‹å®ã€æ–°é—»ã€çŸ¥è¯†ï¼Œneeds_search è®¾ä¸º trueï¼Œå¹¶ç”Ÿæˆæœ€å¥½çš„ä¸­æ–‡æœç´¢å…³é”®è¯ã€‚
4. å¦‚æœç”¨æˆ·è¡¨è¾¾äº†ç»“æŸã€å‘Šåˆ«ã€åœæ­¢å¯¹è¯çš„æ„å›¾ï¼Œè¯·å°† is_exit è®¾ä¸º trueã€‚
5. è¯·åŠ¡å¿…è¿”å›åˆæ³•çš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å« Markdown ä»£ç å—ï¼ˆå¦‚ ```jsonï¼‰ã€‚

JSON æ ¼å¼ç¤ºä¾‹ï¼š
{{
    "summary": "åˆ†ææ‘˜è¦",
    "search_query": "...",
    "needs_search": true/false,
    "is_exit": true/false
}}
"""

PROMPT_ANSWER = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹ã€‚
å½“å‰æ—¶é—´æ˜¯ï¼š{current_date}

è¯·åŸºäºä»¥ä¸‹æä¾›çš„ã€æœç´¢ç»“æœä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
ç”¨æˆ·é—®é¢˜: {user_query}

ã€æœç´¢ç»“æœä¸Šä¸‹æ–‡ã€‘:
{search_context}

è¦æ±‚ï¼š
1. **å‡†ç¡®æ€§**ï¼šä¸¥æ ¼åŸºäºæœç´¢ç»“æœå›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚
2. **å¼•ç”¨**ï¼šåœ¨å›ç­”ä¸­é€‚å½“å¼•ç”¨æ¥æºï¼ˆä¾‹å¦‚ [1], [2]ï¼‰ã€‚
3. **ç»“æ„**ï¼šå¦‚æœå†…å®¹è¾ƒå¤šï¼Œä½¿ç”¨è¦ç‚¹ç¬¦å·åˆ—è¡¨ã€‚
4. **è¡¥å……**ï¼šå¦‚æœæœç´¢ç»“æœæ— æ³•å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®è¯´æ˜ã€‚
5. **å…œåº•**ï¼šå¦‚æœæœç´¢ç»“æœä¸ºç©ºï¼Œè¯·åŸºäºä½ çš„é€šç”¨çŸ¥è¯†å°è¯•å›ç­”ï¼Œå¹¶å‘ŠçŸ¥ç”¨æˆ·è¿™æ˜¯åŸºäºé€šç”¨çŸ¥è¯†ã€‚
"""

# Nodes
# ===================================================
async def analyze_node(state: SearchState) -> dict:
    """åˆ†æç”¨æˆ·æ„å›¾ """
    messages = state["messages"]
    system_prompt = PROMPT_ANALYZE.format(current_date=get_current_date())

    try:
        conversation = [SystemMessage(content=system_prompt)] + messages
        response = await llm.ainvoke(conversation)
        content = response.content.replace("```json", "").replace("```", "").strip()
        data = json.loads(content)
        analysis = QueryAnalysis(**data)
    except Exception as e:
        print(f"âš ï¸ JSON è§£æå¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æœç´¢æ¨¡å¼: {e}")
        analysis = QueryAnalysis(
            summary="è§£æå¤±è´¥ï¼Œé»˜è®¤æœç´¢",
            search_query=messages[-1].content,
            needs_search=True
        )
    return {
        "analysis": analysis,
    }

def _format_search_results(tavily_response:dict) -> str:
    """æ ¼å¼åŒ– Tavily è¿”å›çš„ JSON"""
    results = []
    if tavily_response.get("answer"):
        results.append(f"--- æ™ºèƒ½æ‘˜è¦ ---\n{tavily_response['answer']}\n")

    raw_results = tavily_response.get("results", [])
    if raw_results:
        results.append("--- è¯¦ç»†æ¥æº ---")
        for i, res in enumerate(raw_results[:5], 1):
            title = res.get("title", "æ— æ ‡é¢˜")
            content = res.get("content", "æ— å†…å®¹")
            url = res.get("url", "#")
            results.append(f"[{i}] {title}\næ‘˜è¦: {content}\né“¾æ¥: {url}\n")
    return "\n".join(results) if results else "æ— æœç´¢ç»“æœã€‚"

async def search_node(state: SearchState) -> dict:
    """æ‰§è¡Œæœç´¢"""
    analysis = state["analysis"]
    if not analysis or not analysis.needs_search:
        return {
            "search_context": "æ— éœ€æœç´¢ã€‚",
            "step": "skip_search"
        }

    query = analysis.search_query
    print(f"ğŸ” æ™ºèƒ½æœç´¢: {query}")

    try:
        response = await asyncio.to_thread(
            tavily_client.search,
            query=query,
            include_answer=True,
            max_results=3
        )
        context = _format_search_results(response)
    except Exception as e:
        print(f"æ‰§è¡Œæœç´¢æ—¶å‡ºé”™: {e}")
        context = "æœç´¢å¤±è´¥ï¼Œæ— æ³•è·å–ç»“æœã€‚"

    return {
        "search_context": context,
    }

async def answer_node(state: SearchState) -> dict:
    """ç”Ÿæˆæœ€ç»ˆå›ç­”"""
    analysis = state.get("analysis")
    context = state.get("search_context", "")
    messages = state["messages"]

    user_query = messages[-1].content

    if not analysis or not analysis.needs_search:
        response = await llm.ainvoke(messages)
    else:
        system_msg = PROMPT_ANSWER.format(
            current_date=get_current_date(),
            user_query=user_query,
            search_context=context
        )
        final_msg = [SystemMessage(content=system_msg)] + messages
        response = await llm.ainvoke(final_msg)

    return {
        "messages": [response]
    }

# Graph
# ==================================================
def route(state: SearchState) -> Literal["search", "generate"]:
    """æ ¹æ®åˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥"""
    analysis = state["analysis"]
    if analysis and analysis.needs_search:
        return "search"
    return "generate"

def create_graph():
    workflow = StateGraph(SearchState)

    workflow.add_node("analyze", analyze_node)
    workflow.add_node("search", search_node)
    workflow.add_node("generate", answer_node)

    workflow.add_edge(START, "analyze")
    workflow.add_conditional_edges(
        "analyze",
        route
    )
    workflow.add_edge("search", "generate")
    workflow.add_edge("generate", END)

    return workflow.compile(checkpointer=InMemorySaver())

async def main():
    app = create_graph()
    print("\nğŸŒ LangGraph æ·±åº¦æœç´¢åŠ©æ‰‹å·²å°±ç»ª")
    print("-----------------------------------")
    config = {"configurable": {"thread_id": "user-session-1144"}}

    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
            if user_input.lower() in ["exit", "quit", "q", "å†è§"]:
                print("ğŸ‘‹ å†è§ï¼")
                break
            if not user_input:
                continue
            print("ğŸ¤– æ€è€ƒä¸­...", end="", flush=True)

            should_exit = False
            async for event in app.astream(
                {"messages": [HumanMessage(content=user_input)]},
                config=config
            ):
                for node_name, state_update in event.items():
                    if node_name == "analyze":
                        analysis = state_update.get("analysis")
                        if analysis.is_exit:
                            should_exit = True
                        elif analysis.needs_search:
                            print(f"\n   â†³ ğŸ¯ æ„å›¾è¯†åˆ«: {analysis.summary}")
                            print(f"   â†³ ğŸ”‘ æœç´¢å…³é”®è¯: {analysis.search_query}")
                    elif node_name == "search":
                        context = state_update.get("search_context", "")
                        if context:
                            print(f"   â†³ ğŸ“š æ£€ç´¢åˆ°èµ„æ–™ (é•¿åº¦: {len(context)} å­—ç¬¦)")
                    elif node_name == "generate":
                        last_msg = state_update["messages"][-1]
                        print(f"\nğŸ¤– å›ç­”: {last_msg.content}")
                        print("-"*40)
            if should_exit:
                print("ğŸ‘‹ æ™ºèƒ½åŠ©æ‰‹ä¸‹çº¿ã€‚")
                break
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâ— å‡ºé”™äº†: {e}")

if __name__ == "__main__":
    asyncio.run(main())